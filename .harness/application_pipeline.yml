pipeline:
  name: University Application Intelligence System v2.0 - Harness
  identifier: university_application_intelligence
  projectIdentifier: personal_publicdata
  orgIdentifier: default
  tags: 
    system: "application-intelligence"
    version: "2.0.0"
  
  properties:
    ci:
      codebase:
        connectorRef: github_connector
        repoName: personal-publicdata
        build: <+input>

  stages:
    - stage:
        name: Environment Setup and Dependencies
        identifier: setup
        description: Setup Python environment and install dependencies
        type: CI
        spec:
          cloneCodebase: true
          platform:
            os: Linux
            arch: Amd64
          runtime:
            type: Cloud
            spec: {}
          execution:
            steps:
              - step:
                  type: Run
                  name: System Information
                  identifier: system_info
                  spec:
                    shell: Bash
                    command: |
                      echo "üîß System Setup - Application Intelligence System v2.0"
                      echo "Date: $(date)"
                      echo "Python version: $(python3 --version)"
                      echo "Pip version: $(pip3 --version)"
                      echo "Git commit: $(git rev-parse --short HEAD)"
                      
              - step:
                  type: Run
                  name: Install Dependencies
                  identifier: install_deps
                  spec:
                    shell: Bash
                    command: |
                      echo "üì¶ Installing Python dependencies..."
                      cd build_scripts
                      
                      # Install core dependencies
                      pip3 install --upgrade pip
                      pip3 install PyYAML requests beautifulsoup4 selenium
                      pip3 install pandas matplotlib seaborn
                      pip3 install nltk textblob scikit-learn
                      pip3 install python-dateutil pytz
                      pip3 install PyGithub
                      pip3 install rich structlog
                      pip3 install tqdm click
                      pip3 install jsonschema
                      pip3 install GitPython
                      
                      echo "‚úÖ Core dependencies installed"
                      
                      # Try to install optional dependencies
                      echo "üì¶ Installing optional dependencies..."
                      pip3 install selenium webdriver-manager || echo "‚ö†Ô∏è Selenium dependencies failed"
                      pip3 install lxml html5lib || echo "‚ö†Ô∏è HTML parsing libraries failed"
                      pip3 install pillow opencv-python || echo "‚ö†Ô∏è Image processing libraries failed"
                      pip3 install markdown || echo "‚ö†Ô∏è Markdown processing failed"
                      
                      echo "üìã Installed packages:"
                      pip3 list | grep -E "(PyYAML|requests|beautifulsoup4|pandas|nltk|PyGithub)"

    - stage:
        name: Data Collection and Validation
        identifier: data_pipeline
        description: Automated data collection and validation
        type: CI
        spec:
          cloneCodebase: true
          platform:
            os: Linux
            arch: Amd64
          runtime:
            type: Cloud
            spec: {}
          execution:
            steps:
              - step:
                  type: Run
                  name: Validate Configuration
                  identifier: validate_config
                  spec:
                    shell: Bash
                    command: |
                      echo "üîç Validating system configuration..."
                      
                      # Check if required files exist
                      if [ ! -f "source_data/schools.yml" ]; then
                        echo "‚ùå schools.yml not found"
                        exit 1
                      fi
                      
                      # Validate YAML syntax
                      python3 -c "import yaml; yaml.safe_load(open('source_data/schools.yml'))"
                      echo "‚úÖ schools.yml is valid"
                      
                      if [ -f "source_data/recommenders.yml" ]; then
                        python3 -c "import yaml; yaml.safe_load(open('source_data/recommenders.yml'))"
                        echo "‚úÖ recommenders.yml is valid"
                      fi
                      
                      # List available schools
                      echo "üìã Available schools:"
                      cd build_scripts
                      python3 generate_docs.py --list
                      
              - step:
                  type: Run
                  name: Data Collection - Web Scraping
                  identifier: data_collection
                  spec:
                    shell: Bash
                    command: |
                      echo "üîç Starting automated data collection..."
                      
                      # Set up environment variables for web scraping
                      export DISPLAY=:99
                      export PYTHONPATH="${PYTHONPATH}:$(pwd)"
                      
                      # Run data collection with error handling
                      cd data_collection
                      
                      echo "üï∑Ô∏è Running university data scraper..."
                      timeout 300 python3 scraper.py || {
                        echo "‚ö†Ô∏è Scraper timeout or error, continuing with existing data..."
                      }
                      
                      # Check if live data was created
                      if [ -f "../source_data/schools_live_data.yml" ]; then
                        echo "‚úÖ Live data collection successful"
                        echo "üìä Live data summary:"
                        python3 -c "
                        import yaml
                        with open('../source_data/schools_live_data.yml') as f:
                            data = yaml.safe_load(f)
                            schools = data.get('schools_live_data', [])
                            print(f'Collected data for {len(schools)} schools')
                            for school in schools[:3]:
                                conf = school.get('confidence_score', 0)
                                print(f'  {school[\"school_id\"]}: {conf:.1%} confidence')
                        "
                      else
                        echo "‚ö†Ô∏è No live data collected, using static configuration"
                      fi
                      
              - step:
                  type: Run
                  name: Data Validation and Eligibility Check
                  identifier: validation
                  spec:
                    shell: Bash
                    command: |
                      echo "‚úÖ Starting comprehensive data validation..."
                      
                      cd data_collection
                      export PYTHONPATH="${PYTHONPATH}:$(pwd)/.."
                      
                      echo "üîç Running application eligibility validator..."
                      python3 validator.py
                      
                      # Check validation results
                      if [ -f "../final_applications/validation_report.md" ]; then
                        echo "‚úÖ Validation completed successfully"
                        echo "üìä Validation summary:"
                        
                        # Extract key metrics from validation report
                        cd ../final_applications
                        python3 -c "
                        import json
                        try:
                            with open('validation_results.json') as f:
                                data = json.load(f)
                                results = data.get('results', {})
                                
                                eligible = sum(1 for r in results.values() if r.get('overall_status') == 'ELIGIBLE')
                                warning = sum(1 for r in results.values() if r.get('overall_status') == 'WARNING')
                                ineligible = sum(1 for r in results.values() if r.get('overall_status') == 'INELIGIBLE')
                                total = len(results)
                                
                                print(f'  ‚úÖ Eligible: {eligible}/{total} ({eligible/total:.1%})')
                                print(f'  ‚ö†Ô∏è  Warning: {warning}/{total}')
                                print(f'  ‚ùå Ineligible: {ineligible}/{total}')
                        except Exception as e:
                            print(f'Could not parse validation results: {e}')
                        "
                      else
                        echo "‚ùå Validation failed"
                        exit 1
                      fi

    - stage:
        name: Intelligence and Analysis
        identifier: intelligence
        description: Academic intelligence and opportunity analysis
        type: CI
        spec:
          cloneCodebase: true
          platform:
            os: Linux
            arch: Amd64
          runtime:
            type: Cloud
            spec: {}
          execution:
            steps:
              - step:
                  type: Run
                  name: Academic Intelligence Gathering
                  identifier: academic_radar
                  spec:
                    shell: Bash
                    command: |
                      echo "üî¨ Starting academic intelligence gathering..."
                      
                      cd analysis
                      export PYTHONPATH="${PYTHONPATH}:$(pwd)/.."
                      
                      # Run academic radar with timeout
                      echo "üéØ Analyzing research opportunities and professor networks..."
                      timeout 180 python3 academic_radar.py || {
                        echo "‚ö†Ô∏è Academic radar timeout, continuing with basic analysis..."
                      }
                      
                      # Check results
                      if [ -f "../final_applications/academic_intelligence_report.md" ]; then
                        echo "‚úÖ Academic intelligence completed"
                        
                        # Show summary
                        python3 -c "
                        import json
                        try:
                            with open('../final_applications/academic_intelligence.json') as f:
                                data = json.load(f)
                                summary = data.get('summary', {})
                                print(f'üìä Academic Intelligence Summary:')
                                print(f'  Schools: {summary.get(\"total_schools\", 0)}')
                                print(f'  Professors: {summary.get(\"total_professors\", 0)}')
                                print(f'  Publications: {summary.get(\"total_publications\", 0)}')
                                print(f'  GitHub Repos: {summary.get(\"total_repositories\", 0)}')
                        except:
                            print('Academic intelligence data not available')
                        "
                      else
                        echo "‚ö†Ô∏è Academic intelligence not completed, continuing..."
                      fi
              
              - step:
                  type: Run
                  name: Generate Monitoring Dashboard
                  identifier: dashboard
                  spec:
                    shell: Bash
                    command: |
                      echo "üìä Generating application monitoring dashboard..."
                      
                      cd monitoring
                      export PYTHONPATH="${PYTHONPATH}:$(pwd)/.."
                      
                      python3 dashboard.py
                      
                      if [ -f "../final_applications/application_dashboard.md" ]; then
                        echo "‚úÖ Dashboard generation successful"
                        
                        # Show dashboard summary
                        head -20 ../final_applications/application_dashboard.md
                      else
                        echo "‚ùå Dashboard generation failed"
                        exit 1
                      fi

    - stage:
        name: Document Generation
        identifier: document_generation
        description: Generate customized application documents
        type: CI
        spec:
          cloneCodebase: true
          platform:
            os: Linux
            arch: Amd64
          runtime:
            type: Cloud
            spec: {}
          execution:
            steps:
              - step:
                  type: Run
                  name: Generate Application Documents
                  identifier: generate_docs
                  spec:
                    shell: Bash
                    command: |
                      echo "üìù Generating customized application documents..."
                      
                      cd build_scripts
                      export PYTHONPATH="${PYTHONPATH}:$(pwd)/.."
                      
                      # Determine which schools to generate based on validation results
                      python3 -c "
                      import json
                      import sys
                      
                      try:
                          with open('../final_applications/validation_results.json') as f:
                              data = json.load(f)
                              results = data.get('results', {})
                              
                          # Get eligible and warning schools
                          priority_schools = [
                              school_id for school_id, result in results.items()
                              if result.get('overall_status') in ['ELIGIBLE', 'WARNING']
                          ]
                          
                          print('Priority schools for document generation:')
                          for school in priority_schools:
                              print(f'  - {school}')
                              
                          # Save school list for next step
                          with open('priority_schools.txt', 'w') as f:
                              f.write(' '.join(priority_schools))
                              
                      except Exception as e:
                          print(f'Using all active schools: {e}')
                          with open('priority_schools.txt', 'w') as f:
                              f.write('all')
                      "
                      
                      # Generate documents
                      if [ -f "priority_schools.txt" ]; then
                        SCHOOLS=$(cat priority_schools.txt)
                        if [ "$SCHOOLS" = "all" ]; then
                          echo "üìù Generating documents for all active schools..."
                          python3 generate_docs.py --all
                        else
                          echo "üìù Generating documents for priority schools..."
                          for school in $SCHOOLS; do
                            echo "  Generating for $school..."
                            python3 generate_docs.py --school "$school"
                          done
                        fi
                      else
                        echo "üìù Generating documents for all active schools..."
                        python3 generate_docs.py --all
                      fi
                      
                      # Document generation summary
                      echo "üìä Document Generation Summary:"
                      find ../final_applications -name "*.md" -path "*/CV_*" | wc -l | xargs echo "  CV documents generated:"
                      find ../final_applications -name "*.md" -path "*/SOP_*" | wc -l | xargs echo "  SOP documents generated:"

    - stage:
        name: Notifications and Task Management
        identifier: notifications
        description: Process alerts and create GitHub issues
        type: CI
        spec:
          cloneCodebase: true
          platform:
            os: Linux
            arch: Amd64
          runtime:
            type: Cloud
            spec: {}
          execution:
            steps:
              - step:
                  type: Run
                  name: Process Alerts and Notifications
                  identifier: alerts
                  spec:
                    shell: Bash
                    command: |
                      echo "üîî Processing alerts and notifications..."
                      
                      cd notifications
                      export PYTHONPATH="${PYTHONPATH}:$(pwd)/.."
                      
                      # Set GitHub token if available
                      if [ -n "$GITHUB_TOKEN" ]; then
                        export GITHUB_TOKEN="$GITHUB_TOKEN"
                        echo "‚úÖ GitHub token configured for issue creation"
                      else
                        echo "‚ö†Ô∏è No GitHub token - issues will be logged but not created"
                      fi
                      
                      # Run notification system
                      python3 alert_system.py
                      
                      # Check results
                      if [ -f "../final_applications/alert_summary.json" ]; then
                        echo "‚úÖ Alert processing completed"
                        
                        # Show alert summary
                        python3 -c "
                        import json
                        try:
                            with open('../final_applications/alert_summary.json') as f:
                                data = json.load(f)
                                
                            print('üìä Alert Summary:')
                            print(f'  Total alerts: {data.get(\"total_alerts\", 0)}')
                            print(f'  GitHub issues created: {data.get(\"created_github_issues\", 0)}')
                            
                            by_severity = data.get('alerts_by_severity', {})
                            for severity, count in by_severity.items():
                                print(f'  {severity}: {count}')
                                
                        except Exception as e:
                            print(f'Could not parse alert summary: {e}')
                        "
                      else
                        echo "‚ö†Ô∏è Alert processing completed with no summary"
                      fi
                      
              - step:
                  type: Run
                  name: Master Controller Summary
                  identifier: master_summary
                  spec:
                    shell: Bash
                    command: |
                      echo "üéØ Running master controller for final summary..."
                      
                      cd build_scripts
                      export PYTHONPATH="${PYTHONPATH}:$(pwd)/.."
                      
                      # Run master controller in report mode
                      echo "üìã Generating comprehensive execution report..."
                      python3 master_controller.py --save-report --quick
                      
                      if [ -f "../final_applications/execution_report.md" ]; then
                        echo "‚úÖ Master execution report generated"
                        
                        echo "üìä Final System Status:"
                        echo "========================================"
                        
                        # Show key sections of the execution report
                        grep -A 10 "## üìä Pipeline Results" ../final_applications/execution_report.md || echo "Report sections not found"
                        
                        echo ""
                        echo "üìÅ Generated Files:"
                        ls -la ../final_applications/*.md ../final_applications/*.json 2>/dev/null || echo "No files generated"
                        
                      else
                        echo "‚ö†Ô∏è Master execution report not generated"
                      fi

    - stage:
        name: Artifact Management and Deployment
        identifier: artifacts
        description: Save and deploy generated artifacts
        type: CI
        spec:
          cloneCodebase: true
          platform:
            os: Linux
            arch: Amd64
          runtime:
            type: Cloud
            spec: {}
          execution:
            steps:
              - step:
                  type: Run
                  name: Organize Generated Files
                  identifier: organize_files
                  spec:
                    shell: Bash
                    command: |
                      echo "üìÅ Organizing generated artifacts..."
                      
                      # Create artifacts directory
                      mkdir -p artifacts/reports artifacts/documents artifacts/data
                      
                      # Copy reports
                      if [ -f "final_applications/validation_report.md" ]; then
                        cp final_applications/validation_report.md artifacts/reports/
                        echo "‚úÖ Validation report archived"
                      fi
                      
                      if [ -f "final_applications/application_dashboard.md" ]; then
                        cp final_applications/application_dashboard.md artifacts/reports/
                        echo "‚úÖ Dashboard archived"
                      fi
                      
                      if [ -f "final_applications/academic_intelligence_report.md" ]; then
                        cp final_applications/academic_intelligence_report.md artifacts/reports/
                        echo "‚úÖ Academic intelligence report archived"
                      fi
                      
                      if [ -f "final_applications/execution_report.md" ]; then
                        cp final_applications/execution_report.md artifacts/reports/
                        echo "‚úÖ Execution report archived"
                      fi
                      
                      # Copy structured data
                      if [ -f "final_applications/validation_results.json" ]; then
                        cp final_applications/validation_results.json artifacts/data/
                      fi
                      
                      if [ -f "final_applications/academic_intelligence.json" ]; then
                        cp final_applications/academic_intelligence.json artifacts/data/
                      fi
                      
                      # Copy application documents
                      find final_applications -name "*.md" -path "*/CV_*" -o -name "*.md" -path "*/SOP_*" | while read file; do
                        if [ -f "$file" ]; then
                          cp "$file" artifacts/documents/
                        fi
                      done
                      
                      # Create manifest
                      cat > artifacts/MANIFEST.txt << EOF
                      University Application Intelligence System v2.0
                      Generated: $(date)
                      Commit: $(git rev-parse --short HEAD)
                      Branch: $(git rev-parse --abbrev-ref HEAD)
                      Pipeline ID: <+pipeline.sequenceId>
                      
                      Files Generated:
                      EOF
                      
                      find artifacts -type f | sort >> artifacts/MANIFEST.txt
                      
                      echo "üìä Artifact Summary:"
                      echo "  Reports: $(ls artifacts/reports/*.md 2>/dev/null | wc -l)"
                      echo "  Documents: $(ls artifacts/documents/*.md 2>/dev/null | wc -l)"
                      echo "  Data Files: $(ls artifacts/data/*.json 2>/dev/null | wc -l)"
                      
              - step:
                  type: SaveCacheS3
                  name: Archive Artifacts
                  identifier: save_artifacts
                  spec:
                    connectorRef: aws_connector
                    region: us-east-1
                    bucket: university-application-artifacts
                    key: intelligence-system/<+pipeline.sequenceId>-<+codebase.commitSha>
                    sourcePaths:
                      - artifacts/
                    archiveFormat: Tar
                  when:
                    stageStatus: Success
                  failureStrategies:
                    - onFailure:
                        errors:
                          - AllErrors
                        action:
                          type: Ignore

  # Enhanced trigger configuration
  triggers:
    - trigger:
        name: Scheduled Daily Intelligence Run
        identifier: daily_intelligence
        enabled: true
        description: Daily automated intelligence gathering and analysis
        type: Cron
        spec:
          type: Cron
          spec:
            expression: "0 6 * * *"  # 6 AM UTC daily
            inputYaml: |
              pipeline_mode: "full"
              skip_scraping: false

    - trigger:
        name: Source Data Changes
        identifier: source_data_changes
        enabled: true
        description: Trigger when source data is modified
        type: Webhook
        spec:
          type: Github
          spec:
            type: PullRequest
            spec:
              connectorRef: github_connector
              autoAbortPreviousExecutions: false
              payloadConditions:
                - key: changedFiles
                  operator: Contains
                  value: source_data/
              headerConditions: []
              actions:
                - opened
                - synchronize
                - reopened
            inputYaml: |
              pipeline_mode: "quick"
              skip_scraping: true

    - trigger:
        name: Template Updates
        identifier: template_updates
        enabled: true
        description: Trigger when templates are modified
        type: Webhook
        spec:
          type: Github
          spec:
            type: Push
            spec:
              connectorRef: github_connector
              autoAbortPreviousExecutions: false
              payloadConditions:
                - key: changedFiles
                  operator: Contains
                  value: templates/
              headerConditions: []
            inputYaml: |
              pipeline_mode: "docs_only"

    - trigger:
        name: Manual Full Pipeline
        identifier: manual_full
        enabled: true
        description: Manual trigger for complete intelligence pipeline
        type: Manual
        spec:
          inputYaml: |
            pipeline_mode: "full"
            skip_scraping: false

  # Pipeline variables
  variables:
    - name: pipeline_mode
      type: String
      description: Pipeline execution mode (full, quick, docs_only)
      value: "quick"
    - name: skip_scraping
      type: String
      description: Skip web scraping to speed up execution
      value: "false"
    - name: target_schools
      type: String
      description: Specific schools to process (comma-separated)
      value: ""
    - name: notification_level
      type: String
      description: Notification verbosity level
      value: "normal"

  # Enhanced notification rules
  notificationRules:
    - name: Pipeline Success Notification
      identifier: success_notification
      pipelineEvents:
        - type: PipelineSuccess
      notificationMethod:
        type: Email
        spec:
          userGroups: []
          recipients:
            - admin@dennisleehappy.org
          subject: "‚úÖ Application Intelligence Pipeline Completed Successfully"
          body: |
            The University Application Intelligence System pipeline has completed successfully.
            
            Pipeline ID: <+pipeline.sequenceId>
            Execution Time: <+pipeline.startTs>
            Commit: <+codebase.commitSha>
            
            Generated artifacts are available in the S3 bucket.
            
            Best regards,
            Application Intelligence System
      enabled: true
    
    - name: Pipeline Failure Notification
      identifier: failure_notification
      pipelineEvents:
        - type: PipelineFailed
      notificationMethod:
        type: Email
        spec:
          userGroups: []
          recipients:
            - admin@dennisleehappy.org
          subject: "‚ùå Application Intelligence Pipeline Failed"
          body: |
            The University Application Intelligence System pipeline has failed.
            
            Pipeline ID: <+pipeline.sequenceId>
            Failed Stage: <+pipeline.stage.name>
            Error Details: <+pipeline.stage.spec.execution.steps.step.failureStrategies>
            
            Please check the pipeline logs for detailed error information.
            
            Best regards,
            Application Intelligence System
      enabled: true

    - name: High Priority Alerts
      identifier: high_priority_alerts
      pipelineEvents:
        - type: PipelineSuccess
      notificationMethod:
        type: Slack
        spec:
          webhookUrl: <+secrets.getValue("slack_webhook_url")>
          message: |
            üéì *Application Intelligence Update*
            
            Pipeline completed with new high-priority alerts detected.
            Check GitHub Issues for action items.
            
            Pipeline: <+pipeline.sequenceId>
      enabled: false  # Enable when Slack webhook is configured
      conditions:
        - key: pipeline.variables.high_priority_alerts
          operator: Equals
          value: "true"
